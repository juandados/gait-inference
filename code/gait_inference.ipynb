{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Getting Features and Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parsing xml annotation files**. Make sure the pose annotation files are in CVAT XML 1.1 for videos format and are located in the folder `data_rf/annotations/`. The annotation and video filenames should match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "annotation_parser = AnnotationParser()\n",
    "base_path = \"../data_rf\"\n",
    "video_names = [\"VID_20200325_172105.mp4\",\"VID_20200320_191355.mp4\",\"VID_20200325_171448.mp4\"]\n",
    "video_names = [\"VID_20200325_172105.mp4\"]\n",
    "\n",
    "annotations_df = pd.DataFrame({})\n",
    "for video_name in video_names:\n",
    "    video_path = \"{}/videos/{}\".format(base_path,video_name)\n",
    "    annotation_path = \"{}/annotations/{}.xml\".format(base_path,video_name.split('.')[0])\n",
    "    annotation_df = annotation_parser.parse(annotation_file_name=annotation_path)\n",
    "    annotation_df = annotation_df.loc[annotation_df['activity']!='none']\n",
    "    annotation_df[\"video_name\"]=video_name\n",
    "    annotations_df = annotations_df.append(annotation_df)\n",
    "    \n",
    "data_names_df = pd.DataFrame(annotations_df.set_index([\"video_name\",\"frame\"]).index.unique())\n",
    "data_names_df = data_names_df.apply(lambda x: pd.Series({\"video_name\":x[0][0],\"frame\":x[0][1]}),axis=1)\n",
    "print(\"{} video frames annotated for pose estimation\".format(data_names_df.shape[0]))\n",
    "data_names_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cropping people from video frames.** It is saving cropped images in folder `data_rf/images/`. It may be very time consuming, that is why the cutting flag is set to False, set it to True only if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "t0 = time()\n",
    "cutting_flag = False # Change This for cutting\n",
    "if cutting_flag:\n",
    "    cutting_details = []\n",
    "    dest_path = \"{}/images\".format(base_path)\n",
    "    if not os.path.exists(dest_path):\n",
    "        print(\"Creating directory {}\".format(dest_path))\n",
    "        os.makedirs(dest_path)\n",
    "    for video_name in data_names_df['video_name'].unique():\n",
    "        frame_counters = data_names_df.set_index('video_name').loc[video_name,'frame'].values\n",
    "        frame_extractor = FrameExtractor(\"{}/videos/{}\".format(base_path,video_name))\n",
    "        img_prefix = video_name.split('.')[0]\n",
    "        cutting_details.append(frame_extractor.extract_person_from_frame_list(frame_counters=frame_counters,\n",
    "                                                                              img_name=img_prefix, dest_path=dest_path))\n",
    "    cutting_details_df = pd.DataFrame(list(np.hstack(cutting_details)))\n",
    "    cutting_details_df.to_csv('{}/cutting_details.csv'.format(base_path),index=False)\n",
    "else:\n",
    "    cutting_details_df = pd.read_csv('{}/cutting_details.csv'.format(base_path))\n",
    "    cutting_details_df['cutting_rect'] = cutting_details_df['cutting_rect'].apply(json.loads)\n",
    "print('total_time {}'.format(time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Simple Baselines Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up the simple baselines model architecture.** The pose estimation model follows most of the [[Xiao-18]](https://arxiv.org/pdf/1804.06208.pdf) gidelines for pose estimation (no optical flow implemented yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.utils import shuffle\n",
    "from keras import optimizers, applications\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from keras.models import Sequential, Model,load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, GlobalAveragePooling2D\n",
    "from keras.layers import Conv2DTranspose, BatchNormalization\n",
    "from keras.callbacks import LearningRateScheduler, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import TensorBoard, CSVLogger\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "img_height, img_width = (256,192)\n",
    "base_model = ResNet50(weights='imagenet',include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "output = base_model.output\n",
    "output = Conv2DTranspose(filters=256, kernel_size=4, strides=(2, 2), activation='relu')(output)\n",
    "output = BatchNormalization()(output)\n",
    "output = Conv2DTranspose(filters=256, kernel_size=4, strides=(2, 2), activation='relu')(output)\n",
    "output = BatchNormalization()(output)\n",
    "output = Conv2DTranspose(filters=256, kernel_size=4, strides=(2, 2), activation='relu')(output)\n",
    "output = BatchNormalization()(output)\n",
    "output = Conv2D(filters=8, kernel_size=1, activation='linear')(output)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trainning with data augmentation.** Setting up the image data generators, callbacks used for training and performing training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_names = data_names_df.apply(lambda x:\"{}_{}.jpg\".format(x['video_name'].split('.')[0],x['frame']),axis=1).values\n",
    "list_IDs_train, list_IDs_test = train_test_split(data_names, test_size=0.15,random_state=0)\n",
    "batch_size = 32\n",
    "# Train iterator\n",
    "image_data_generator_params ={\n",
    "    \"featurewise_center\":True,\n",
    "    \"featurewise_std_normalization\":True,\n",
    "    \"rotation_range\":5,\n",
    "    \"width_shift_range\":0.1,\n",
    "    \"height_shift_range\":0,\n",
    "    \"horizontal_flip\":True}\n",
    "iterator_params = {'dim_input': (256, 192),\n",
    "                   'dim_output': (78, 62),\n",
    "                   'batch_size': batch_size,\n",
    "                   'n_channels_input': 3,\n",
    "                   'n_channels_output': 8,\n",
    "                   'shuffle': True}\n",
    "path_to_images='../data_rf/images/'\n",
    "train_iterator = PoseIterator(list_IDs_train, annotations_df, cutting_details_df, path_to_images, \n",
    "                 batch_size=32,dim_input=(256, 192), dim_output=(78, 62), n_channels_input=3, \n",
    "                 n_channels_output=8, shuffle=True, seed=0, **image_data_generator_params)\n",
    "# Validation iterator\n",
    "image_data_generator_params ={}\n",
    "iterator_params = {'dim_input': (256, 192),\n",
    "                   'dim_output': (78, 62),\n",
    "                   'batch_size': batch_size,\n",
    "                   'n_channels_input': 3,\n",
    "                   'n_channels_output': 8,\n",
    "                   'shuffle': True}\n",
    "path_to_images='../data_rf/images/'\n",
    "validation_iterator = PoseIterator(list_IDs_test, annotations_df, cutting_details_df, path_to_images, \n",
    "                 batch_size=32,dim_input=(256, 192), dim_output=(78, 62), n_channels_input=3, \n",
    "                 n_channels_output=8, shuffle=True, seed=0, **image_data_generator_params)\n",
    "\n",
    "# Setting up Callbacks\n",
    "filepath = \"checkpoints/simple_baseline_weights-{epoch:02d}-{loss:.8f}-{val_loss:.8f}.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='min', restore_best_weights=False)\n",
    "csv_logger = CSVLogger('logs/training.log')\n",
    "tensor_board = TensorBoard(log_dir='logs', update_freq='epoch')\n",
    "reduce_lr_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15,mode='min', min_lr=1e-6)\n",
    "def scheduler(epoch):\n",
    "    if epoch < 10:\n",
    "        return 1e-4\n",
    "    elif epoch < 15:\n",
    "        return 1e-5\n",
    "    else:\n",
    "        return 1e-6\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "callbacks = [checkpoint, early_stop, reduce_lr_on_plateau, tensor_board, csv_logger]\n",
    "\n",
    "history = model.fit_generator(generator=train_iterator, steps_per_epoch=20000 // batch_size,\n",
    "                              validation_data=validation_iterator, validation_steps=400 // batch_size,\n",
    "                              use_multiprocessing=True, workers=6, epochs=1, callbacks=callbacks)\n",
    "\n",
    "# plotting Learning curves\n",
    "df = pd.read_csv('logs/training.log')\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "ax.semilogy(df[['loss','val_loss']].values);\n",
    "ax.grid()\n",
    "ax.legend(['Train loss','Validation loss'],fontsize=14)\n",
    "ax.set_xlabel('Epoch',fontsize=16)\n",
    "ax.set_ylabel(r'$\\ell_{2}$ loss',fontsize=16)\n",
    "ax.tick_params(axis='x', labelsize=14)\n",
    "ax.tick_params(axis='y', labelsize=14)\n",
    "ax2=ax.twinx()\n",
    "ax2.semilogy(df[['lr']].values,color='red')\n",
    "ax2.set_ylabel('learing rate',color='red',fontsize=16)\n",
    "ax2.tick_params(axis='y', labelsize=14)\n",
    "fig.savefig('figs/train_results.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Only Inference for new Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_existing_model = False\n",
    "if load_existing_model:\n",
    "    model.load_weights('checkpoints/simple_baseline_weights-47-0.00001083-0.00003969.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../data_rf/images\"\n",
    "filename = \"testing.png\"\n",
    "image_path = \"{}/{}\".format(base_path, filename) # New Image path\n",
    "imgcv = cv2.imread(image_path)\n",
    "\n",
    "# Preprocesing input X\n",
    "yolo_detector = Yolo()\n",
    "X, width_scale, height_scale, cutting_rect = yolo_detector.extract_person(imgcv)\n",
    "X = cv2.cvtColor(img_to_array(X)/255, cv2.COLOR_BGR2RGB)\n",
    "X = np.expand_dims(X, axis=0)\n",
    "\n",
    "# Model Prediction\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Plotting Results\n",
    "ORDERED_LABELS = ['r_toe','l_toe','r_ankle','l_ankle','r_knee','l_knee','r_hip','l_hip']\n",
    "f, ax = plt.subplots(2,4,figsize=(16,8))\n",
    "XX = cv2.resize(X[0,:,:,:], dsize=(62,78), interpolation=cv2.INTER_CUBIC)\n",
    "for joint_ind in range(8):\n",
    "    ax[joint_ind//4][joint_ind%4].imshow(XX)\n",
    "    ax[joint_ind//4][joint_ind%4].imshow(y_pred[0,:,:,joint_ind],alpha=0.4)\n",
    "    ax[joint_ind//4][joint_ind%4].set_title('{}'.format(ORDERED_LABELS[joint_ind]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference and Decoding Predictions for new Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_existing_model = False\n",
    "if load_existing_model:\n",
    "    model.load_weights('checkpoints/simple_baseline_weights-47-0.00001083-0.00003969.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../data_rf/images\"\n",
    "filename = \"testing.png\"\n",
    "image_path = \"{}/{}\".format(base_path, filename)\n",
    "imgcv = cv2.imread(image_path)\n",
    "\n",
    "yolo_detector = Yolo()\n",
    "X, width_scale, height_scale, cutting_rect = yolo_detector.extract_person(imgcv)\n",
    "X = cv2.cvtColor(img_to_array(X)/255, cv2.COLOR_BGR2RGB)\n",
    "f, ax = plt.subplots(2,2,figsize=(8,12))\n",
    "XX = cv2.resize(X, dsize=(62,78), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "for joint_ind in range(7):\n",
    "    coords = np.where(y_pred==np.max(y_pred[0,:,:,joint_ind]))\n",
    "    coords_0 = [coord[0] for coord in coords][1:3]\n",
    "    coords_1 = np.dot(np.diag([256/78,192/62]),coords_0)\n",
    "    coords_2 = np.dot(np.diag([1/height_scale,1/width_scale]),coords_1).astype(int)\n",
    "    coords_3 = [cutting_rect[1]+coords_2[0],cutting_rect[0]+coords_2[1]]\n",
    "    # Resolution 78 x 62\n",
    "    ax[0][0].imshow(XX)\n",
    "    ax[0][0].plot(coords_0[1],coords_0[0],'ro')\n",
    "    # Resolution 256 x 192\n",
    "    ax[0][1].imshow(X)\n",
    "    ax[0][1].plot(coords_1[1],coords_1[0],'ro')\n",
    "    # Original Resolution\n",
    "    ax[1][0].imshow(cv2.cvtColor(img_to_array(array_to_img(imgcv).crop(cutting_rect)),cv2.COLOR_BGR2RGB)/255)\n",
    "    ax[1][0].plot(coords_2[1],coords_2[0],'ro')\n",
    "    # Original Coordinates\n",
    "    ax[1][1].imshow(cv2.cvtColor(img_to_array(imgcv)/255, cv2.COLOR_BGR2RGB))\n",
    "    ax[1][1].plot(coords_3[1],coords_3[0],'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gait Estimation: Transfemoral Amputation Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing some useful packages\n",
    "from numpy.linalg import norm\n",
    "from utils import AnnotationParser, PoseEstimator\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Getting Features and Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collecting Gait Data.** We use the previously trained model to collect data. The gait videos and gait annotations must be located in the `data_rf/videos/` and `../data_rf/gait_annotations/` folders respectively. **Note:** This is time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collect_new_gait_data_flag = True #Change this if needed\n",
    "\n",
    "if collect_new_gait_data_flag:\n",
    "    annotation_parser = AnnotationParser()\n",
    "    print('--> annotattion parser loaded!!!')\n",
    "    # Change model path in case other pose estimator was trained\n",
    "    pose_estimator = PoseEstimator(model_path=\"checkpoints/simple_baseline_weights-47-0.00001083-0.00003969.h5\")\n",
    "    print('--> pose estimator loaded!!!')\n",
    "    t0 = time()\n",
    "    video_names = ['VID_20200325_171448.mp4','VID_20200330_140612.mp4','VID_20200325_171938.mp4',\n",
    "                   'VID_20200330_141206.mp4']\n",
    "    base_video_path = '../data_rf/videos'\n",
    "    base_gait_annotation_path = '../data_rf/gait_annotations'\n",
    "    one = 0.99999\n",
    "    \n",
    "    gait_details_df = pd.DataFrame({})\n",
    "    ORDERED_LABELS = ['r_toe','l_toe','r_ankle','l_ankle','r_knee','l_knee','r_hip','l_hip']\n",
    "    for video_name in video_names:\n",
    "        annotation_file_name = '{}/{}_gait.xml'.format(base_gait_annotation_path, video_name.split('.mp4')[0])\n",
    "        activities_df = annotation_parser.parse_activities_only(annotation_file_name)\n",
    "        activities_df = activities_df.loc[activities_df['activity']!='none']\n",
    "        frame_counters = activities_df.frame.values\n",
    "        video_path = '{}/{}'.format(base_video_path,video_name)\n",
    "        vid_cap = cv2.VideoCapture(video_path)\n",
    "        activities_df.reset_index(inplace=True,drop=True)\n",
    "        \n",
    "        activities_df['shot']=pd.Series(np.cumsum(np.append([False],np.diff(activities_df['frame'])>1)))\n",
    "        gait_details = []\n",
    "        for frame_counter in frame_counters:\n",
    "            print(frame_counter)\n",
    "            vid_cap.set(cv2.CAP_PROP_POS_FRAMES, frame_counter)\n",
    "            ret, imgcv = vid_cap.read()\n",
    "            joint_coords = pose_estimator.estimate_pose(imgcv)\n",
    "            r_leg_coords = joint_coords[[0,2,4,6],:]\n",
    "            l_leg_coords = joint_coords[[1,3,5,7],:]\n",
    "            r_diff = np.diff(r_leg_coords,axis=0)\n",
    "            l_diff = np.diff(l_leg_coords,axis=0)\n",
    "            # Computing angles\n",
    "            theta_r_ankle = np.arccos(-one*r_diff[0,1]/norm(r_diff[0,:]))\n",
    "            theta_r_knee = np.arccos(-one*r_diff[1,1]/norm(r_diff[1,:]))\n",
    "            theta_r_hip = np.arccos(-one*r_diff[2,1]/norm(r_diff[2,:]))\n",
    "            theta_l_ankle = np.arccos(-one*l_diff[0,1]/norm(l_diff[0,:]))\n",
    "            theta_l_knee = np.arccos(-one*l_diff[1,1]/norm(l_diff[1,:]))\n",
    "            theta_l_hip = np.arccos(-one*l_diff[2,1]/norm(l_diff[2,:]))\n",
    "            coords_dict = dict(zip(ORDERED_LABELS,joint_coords))\n",
    "            angles_dict = {\"theta_r_ankle\": theta_r_ankle,\n",
    "                           \"theta_r_knee\": theta_r_knee,\n",
    "                           \"theta_r_hip\": theta_r_hip,\n",
    "                           \"theta_l_ankle\": theta_l_ankle,\n",
    "                           \"theta_l_knee\": theta_l_knee,\n",
    "                           \"theta_l_hip\":theta_l_hip}\n",
    "            gait_details.append({\"frame\":frame_counter,**coords_dict, **angles_dict})\n",
    "        gait_details_df_temp = pd.DataFrame(gait_details)\n",
    "        gait_details_df_temp = activities_df.merge(gait_details_df_temp,on=\"frame\")\n",
    "        gait_details_df_temp['video_name']=video_name\n",
    "        gait_details_df_temp['activity'] = activities_df['activity']\n",
    "        gait_details_df = gait_details_df.append(gait_details_df_temp,ignore_index=True)\n",
    "    print('total time', time()-t0)\n",
    "    gait_details_df.to_pickle('../data_rf/gait_details.pkl')\n",
    "else:\n",
    "    gait_details_df=pd.read_pickle('../data_rf/gait_details.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing Training Features and Targets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delay_theta_columns(df, periods, theta_columns):\n",
    "    for period in periods:\n",
    "        result = (df[theta_columns]).shift(period)\n",
    "        df = df.merge(result, suffixes=('','_{}'.format(period)),left_index=True, right_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting delayed features\n",
    "theta_columns = ['theta_r_hip', 'theta_l_hip']\n",
    "delayed_gait_details_df = gait_details_df.groupby(['video_name','shot']).apply(lambda x: delay_theta_columns(x,np.arange(1,31),theta_columns))\n",
    "delayed_gait_details_df.dropna(inplace=True)\n",
    "\n",
    "r_regex = re.compile(r'\\w+_hip_(\\w+)((\\_\\d)*)')\n",
    "X_cols = [col for col in delayed_gait_details_df.columns if type(re.match(r_regex, col))!=type(None)]\n",
    "X_cols = [*X_cols, 'theta_l_hip','theta_r_hip']\n",
    "y_cols = ['theta_l_ankle', 'theta_r_ankle', 'theta_l_knee','theta_r_knee']\n",
    "X = delayed_gait_details_df[X_cols]\n",
    "y = delayed_gait_details_df[y_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "train_index = delayed_gait_details_df['shot']!=1\n",
    "test_index = delayed_gait_details_df['shot']==1\n",
    "X_train_df = X.loc[train_index]\n",
    "X_test_df = X.loc[test_index]\n",
    "y_train_df = y.loc[train_index]\n",
    "y_test_df = y.loc[test_index]\n",
    "gait_details_train_df = delayed_gait_details_df.loc[train_index, gait_details_df.columns]\n",
    "gait_details_test_df = delayed_gait_details_df.loc[test_index, gait_details_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape features the inputs (X) are reshaped into the 3D format expected by LSTMs, \n",
    "# namely [samples, timesteps, features].\n",
    "X_train_values = X_train_df.values\n",
    "X_test_values = X_test_df.values\n",
    "X_train = X_train_values.reshape((X_train_df.shape[0], 1, X_train_df.shape[1]))\n",
    "X_test = X_test_df.values.reshape((X_test_df.shape[0], 1, X_test_df.shape[1]))\n",
    "y_train = y_train_df.values\n",
    "y_test = y_test_df.values\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Sequential Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting Up and Training Sequential Models.** The considered models are linear (as baseline), single layer and multi layer LSTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "def model_gait_generator(model_name='LSTM_single_layer'):\n",
    "    if model_name == 'linear':\n",
    "        model_gait = Sequential()\n",
    "        model_gait.add(Dense(y_train.shape[1],input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        model_gait.compile(loss='mse', optimizer='adam')\n",
    "        return model_gait\n",
    "    if model_name == 'LSTM_single_layer':\n",
    "        model_gait = Sequential()\n",
    "        model_gait.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        model_gait.add(Dense(y_train.shape[1]))\n",
    "        model_gait.compile(loss='mse', optimizer='adam')\n",
    "        return model_gait\n",
    "    if model_name == 'LSTM_multi_layer':\n",
    "        model_gait = Sequential()        \n",
    "        model_gait.add(LSTM(20, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        model_gait.add(LSTM(10, return_sequences=True))\n",
    "        model_gait.add(LSTM(10))\n",
    "        model_gait.add(Dense(y_train.shape[1]))\n",
    "        model_gait.compile(loss='mse', optimizer='adam')\n",
    "        return model_gait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit LSTM_single_layer\n",
    "model_gait_linear = model_gait_generator(model_name='linear')\n",
    "history_linear = model_gait_linear.fit(X_train, np.expand_dims(y_train,1), epochs=200, batch_size=32, validation_data=(X_test, np.expand_dims(y_test,1)), verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit LSTM_single_layer\n",
    "model_gait_single = model_gait_generator(model_name='LSTM_single_layer')\n",
    "history_single = model_gait_single.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test), verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit LSTM_single_layer\n",
    "model_gait_multi = model_gait_generator(model_name='LSTM_multi_layer')\n",
    "history_multi = model_gait_multi.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test), verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,3,figsize=(9,4))\n",
    "ax[0].plot(history_linear.history['loss'], label='train')\n",
    "ax[0].plot(history_linear.history['val_loss'], label='test')\n",
    "ax[0].set_ylim(0,0.8)\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel(r'$\\ell_{2}loss$')\n",
    "ax[0].set_title('Linear Model')\n",
    "ax[1].plot(history_single.history['loss'], label='train')\n",
    "ax[1].plot(history_single.history['val_loss'], label='test')\n",
    "ax[1].set_ylim(0,0.8)\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_title('Single Layer')\n",
    "ax[2].plot(history_multi.history['loss'], label='train')\n",
    "ax[2].plot(history_multi.history['val_loss'], label='test')\n",
    "ax[2].set_ylim(0,0.8)\n",
    "ax[2].legend()\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].set_title('Multi Layer')\n",
    "#plt.tight_layout()\n",
    "f.savefig('figs/LSTM_training_results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_test_pred = model_gait_multi.predict(X_test)\n",
    "y_train_pred = model_gait_multi.predict(X_train)\n",
    "mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "f, ax = plt.subplots(2,4,figsize=(10,5))\n",
    "# Train Plots\n",
    "w = 300\n",
    "ax[0][0].set_ylabel('Train Samples')\n",
    "ax[0][0].plot(y_train_pred[:w,0])\n",
    "ax[0][0].plot(y_train[:w,0])\n",
    "ax[0][0].set_xlabel('sample')\n",
    "ax[0][0].legend(['pred','real'])\n",
    "ax[0][0].set_title(r'$\\theta_{l,ankle}$')\n",
    "ax[0][1].plot(y_train_pred[:w,1])\n",
    "ax[0][1].plot(y_train[:w,1])\n",
    "ax[0][1].set_xlabel('sample')\n",
    "ax[0][1].legend(['pred','real'])\n",
    "ax[0][1].set_title(r'$\\theta_{r,ankle}$')\n",
    "ax[0][2].plot(y_train_pred[:w,2])\n",
    "ax[0][2].plot(y_train[:w,2])\n",
    "ax[0][2].set_xlabel('sample')\n",
    "ax[0][2].legend(['pred','real'])\n",
    "ax[0][2].set_title(r'$\\theta_{l,knee}$')\n",
    "ax[0][3].plot(y_train_pred[:w,3])\n",
    "ax[0][3].plot(y_train[:w,3])\n",
    "ax[0][3].set_xlabel('sample')\n",
    "ax[0][3].legend(['pred','real'])\n",
    "ax[0][3].set_title(r'$\\theta_{r,knee}$')\n",
    "# Test Plots\n",
    "w = 200\n",
    "ax[1][0].set_ylabel('Test Samples')\n",
    "ax[1][0].plot(y_test_pred[:w,0])\n",
    "ax[1][0].plot(y_test[:w,0])\n",
    "ax[1][0].set_xlabel('sample')\n",
    "ax[1][0].legend(['pred','real'])\n",
    "ax[1][0].set_title(r'$\\theta_{l,ankle}$')\n",
    "ax[1][1].plot(y_test_pred[:w,1])\n",
    "ax[1][1].plot(y_test[:w,1])\n",
    "ax[1][1].set_xlabel('sample')\n",
    "ax[1][1].legend(['pred','real'])\n",
    "ax[1][1].set_title(r'$\\theta_{r,ankle}$')\n",
    "ax[1][2].plot(y_test_pred[:w,2])\n",
    "ax[1][2].plot(y_test[:w,2])\n",
    "ax[1][2].set_xlabel('sample')\n",
    "ax[1][2].legend(['pred','real'])\n",
    "ax[1][2].set_title(r'$\\theta_{l,knee}$')\n",
    "ax[1][3].plot(y_test_pred[:w,3])\n",
    "ax[1][3].plot(y_test[:w,3])\n",
    "ax[1][3].set_xlabel('sample')\n",
    "ax[1][3].legend(['pred','real'])\n",
    "ax[1][3].set_title(r'$\\theta_{r,knee}$')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the gif animations are in the extended version of this notebook [simple_base_refactor.ipynb]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
